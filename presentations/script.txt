[Cover]
Good afternoon ladies and gentlemen, welcome to pay attention to my master thesis defense and my name is zhangzijian. The title of my master thesis is Scalable approach for Learning Word Representation. This master thesis is supervised and examined by Avishek and the second examiner is Wolfgang

[2]
At the beginning of my presentation i would like to go through quickly the agenda of my representation. It is naturally separated into three parts, We will begin with the motivation of our research topic, then the problems is stated and some recent related approaches are introduced. Then we will turn to our approach by at first identifying our contribution and then introduce the detail of each phase of our approach. At the end we will see the performance of our final result on some state of art word embedding benchmarks and the timing important timing factor of them. The presentation will be end with the conclusion and some comments on our ongoing work.

[3]
Before all, please allow me to ask you two questions, the first of which is, what is Hannover minus Berlin plus Hamburg? Well according to the time limit, i would to answer it my own, 

[4]
according to the map, Hannover - Berlin + Hamburg should be somewhere near Neubrandenburg, quite intuitive, isn't it? But what about King minus Man plus Woman? On which map this equation can be solved?

[5]
To solve this question, we have to know what is a 'map'. A map, i mean the one on a paper or in an App, is a map, mathematical jargon, from the cities in an area of Earth surface into a 2 dimensional local Euclidean space. Where each city can be represented by latitude and longitude. Well congratulations, even Einstein is bothered by this question for years and we just solve it in seconds! So what if we change 'cities' to 'words' and the 'surface of Earth' to 'language'? Then the semantic information of words could be expressed using their 'latitude' and 'longitude', and this numerical representation can be get by the machines. The question seems solvable!

[6]
The word vector representation framework is one of the approaches to draw this map. We feed the model some copora in natural language, and it spit the vector representation of each words.

[7]
it is a low dimensional representation of sparse word usage, and we can express the similarity of words using the cosine of their vectors, one of the nearest neighbors of man is woman because the are semantically near with each other. The word2vec can also helps to recognize the categories of words with clustering. And also the not-so-simple task like "which word is related to woman in the same way king relates to man" is solvable according to the parallelism. The result is the Queen! Problems are solved, world seems beautiful, right?

[8](practice more!!!!)
Well, not quite. Roughly speaking, the larger the corpus is, the more expressive it should be, and the training time is also longer. Training on a 14 Gigabyte snapshot of English wikidump takes already 36 hours. what about we want to focus on the temporal variation of wikidump? What about we have to train though different sources, like web archive? The vanilla Word2Vec is simply not scalable with respect to large corpus. We lose an efficient approach to speedup the Word2Vec training!

[9]
There are several recent works trying to deal with it. Because the word2vec fundamentally uses stochastic gradient descent as the optimizing framework, one direct approach to accelerate it is make SGD run faster. Therefore the strategies like Hogwilds! blas-3 sgns, cache optimized hierarchical softmax and even column-wise distribution of training were been considered. Not to mention the adaptive learning rate, like sgdm, adagrad and rmsprop. But these methods are either tided to hardware, or are synchronous approaches.

[10]
Therefore we have found a scalable, hardware independent, asynchronous, distributional paradigm, for training word vector representation, which is comparable with respect to the performance on the state of art benchmark. In the meanwhile as shown on the right hand side, the training of our approach is speed up by 10 or even 100 times compared with the baseline. We will introduce our five approaches in the following slides.

[11]
Here is the big picture of our approach. First of all, the original corpus is divided into several sub-corpora, then for each sub-corpus we train the individual model asynchronously and in parallel. Then we merge all the representations from sub-models into a merged model. At last we run the evaluation on the merged model and compare it to the baseline.

[12]
Given this big pictures of our approach, two obvious questions jump immediately in front of us, the first of them is how should we divide the corpus? and the second of them is how should we merge the sub-models into single merged model?

[13]
To solve these questions, we need to get known of how does word2vec framework calculate the word vectors. We focus on the skip gram negative sampling algorithm of word2vec framework. It is literally divided into two parts. In skip gram, within each sentence a window slide through. The word in the center of each window is called the central word w, and the other words in the same window are called the context c. For negative sampling we draw several negative samples c_N s. Then we have to raise the possibility that given w and c is predicted, and lower the possibility that cN s are predicted.

[14](practice more!!!)
According to the work from Levy and Goldberg in 2014, the result of SGNS is nothing more but an implicit factorization of Point-wise mutual information matrix, and this final result depends on the distribution of words w or c and there skipgrams only. So when we divide, we have to preserve the word and skip-gram distribution! Since the skip grams are bounded by the sentences boundary, we model the words and skip grams as balls contained by the boxes named sentences. Then we perform sentence-wise reservoir sampling to sample from the original corpus into sub-corpora uniformly, preserving the word/skip gram distribution.

[15](practice more!!)
So we tried the sampling rate from 1 one hundred thousandth to one tenth. Here is the KL-divergence of word and skip grams distribution from sub-corpus to the original corpus. The blue line is our approach and the yellow line is the trivial block-wise partitioning for the contrast. It's clear that the both distribution KL converge all the way to that of original corpus. When the size of a sub-corpus is around 1 three hundredth both KL-divergence are nearly zero. While yellow lines are either vibrate or too high to converge. Therefore our approach preserves the distribution better than the trivial one. The performance differences between partitioning and sampling will be shown later on in details.

[16](practice more!!!!)
Since we have solved the first question, the sub-models can be trained on those samples. After that, we can gather several different word representations from individual sub-models. What does each one of them consists of? Because the expected word and bigram distribution is generally preserved. Those sub-representations should have some correlation in all of them. Some significant features can be supported by several sub-models and some noise caused by corpus division maybe. Then our goal is to identify all the correlation parts, weighted average the significant features part and filter out the noise.

[17]
As the first combination approach we chose the principal component analysis. The PCA find a bunch of orthogonal principal components from the covariation matrix as the new basis. The variation projected on each of which is maximized. As a simple example showing how does PCA serve our purpose, consider a 1-d embedding in both sub-model A and B, in the sub-model A the word "queen" is more affiliate to 'jack' maybe because the sub-corpus A contains little bit more sentences about poker cards, and the queen B is more similar to woman b. But on the first principal component the queen merge is, as we expected similar to both jack merge and woman merge.

[18]
Since the PCA is only a kind of global linear transformation, some non-linear local information can be lost. Therefore we decided to use the second approach called low rank alignment. In low rank alignment we first find a low rank reconstruction of each sub-representation. Then we perform a non-linear alignment on both manifold, which take both low-rank reconstruction and inter-manifold correlation into consideration. Finally we average the aligned embeddings and use that as our merged model. But this approach, as will be shown later, does not perform so well and is very slow.

[19](practice more!)
Then we come to our experimental setup. As mentioned before, the original corpus we used is the 14 GB English wikidump in 2016. The implementation of word2vec framework is gensim. The cleaner of the corpus is the wikipedia linesentence cleaner built in gensim. For our approach we used the sentence-wise reservoir sampling and in comparison with trivial block-wise sampling. Each sub-corpus is in size of 1% or 10% of the original corpus respectively. For the hyper-parameters of baseline model we follow the suggestion of Levy et al., word frequency threshold is 100, dimension of embedded vectors are 500. And the final training time was 36 hours on the prometheus server of kbs. To evaluate the performance of merged model we performed word similarity, analogy and categorization tasks and state of art dataset for each task.

[20]
We have used the same merging approach, namely the concatenation and pca to merge and to compare the division approach and the size of sub corpora. It is showed on the table that random sampling of size 1/10 of the original corpus performs overall the best,. Even for the 1/100 sub-corpora the random sampling is more robust and yields better results. The two interesting things are firstly sometime our 1/10 sampling even outperforms the baseline. And second of which is that 1/10 sampling is worse than partitioning on rare words dataset. The reason why is when we concatenate all the sub-representations, we used the intersection of all vocabularies, which can the loss of some words. Especially when the words are too rare. Therefore it is less possible that this word may occur in the merged model.

[21]
Then we hold the division approach still and vary the combination approach and size of sub-corpora, it is showed that the pca is over all the best and can be done in minutes, LRA is the slowest and perform not so well. As we know PCA introduce some loss of information, so we add two lines for the concatenation itself and the results shows the concatenation is indeed more robust. But not so good as pca due to its sparsity

[22]
The count of combined sub-models is also a factor that may affect the performance. So we did another experiments to combine one or two or ten sub-models and have seen that we can hit the plateau before combine all the sub-models. This may eventually save the training resource and get the basically the same results. The division approach are all random sampling and pca for combination. Here is the result for 1/10 sub-models and 

[23]
Here is for 1/100 sub-models.

[24]
Last but not the least. As we showed before. The total time of all approaches. basically an m-fold division leads to an m-times speed up. It takes 36 hour for the baseline for training and 3.6 hours for the 1/10 corpus and less than 1 hour for the 1/100 corpus. The LRA approach performs not so well by evaluation nor timing, The sampling and PCA approach is the winner.

[25]
As conclusion, we introduced an approach of scalable learning of word representation. At first sentence-wise sample the sub-corpora and train on them individually, at last combine them using pca. Our approach can speed up the training process M-times by M-fold division. And surprisingly our approach sometimes outperforms the baseline. As for the ongoing work, as you noticed, we used the intersection of vocabularies in all sub-models by model merging. So we are now trying to solve the missing word problem to enhance the expressibility of merged model even more.

 And we have done the experiment, where only the words occurring in the combined model are evaluated, to see whether the performance decreasing is due to lost of words by intersecting. The result of experiment confirmed our hypothesis and the evaluation shows the better result than that on all words. Therefore one of our ongoing work is to inflate the missing vocabulary while merging the sub-models. Which increases the recall of the merged model and keeps the accuracy of the words that already exist.