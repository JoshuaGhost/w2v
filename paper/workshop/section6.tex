\section{Conclusion and future work}
In this paper we provide several configurations to combine models trained using different corpus (fragments) together, and find with the help of \verb|PCA| using consecutive order performs the best. But in the future when this combination process should run on a e.g. Map-Reduce cluster in order to decrease the calculation time one step further, binary order can be then chosen with only slice performance loss. Certainly for optimizing construction process of word embeddings there ere still a lot of other possible approaches may worth trying. For example when we concatenate all the vectors from sub-models together, we get actually a word embedding with dimension $dk$. An auto-encoder can then be used efficiently for dimension reduction \cite{hinton2006reducing}. Furthermore, method like asynchronous VRSGD \cite{keuperasynchronous}, \cite{keuper2015asynchronous} provides the possibility of optimizing SGD in a parallel way. This may help if we want to reform the word embedding fundamentally.
