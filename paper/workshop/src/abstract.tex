\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between training speed and performance.
 
\end{abstract}
