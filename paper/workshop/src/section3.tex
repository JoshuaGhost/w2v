\section{Speed-up Approaches so far}
Training a well-performed natural language model entails huge size of corpora. However, this training process is highly time consuming. For example, the experiment with vanilla configuration mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be conducted. Hyper-parameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count-of-word when building vocabulary from whole corpus may do effect on training time. Also according to the paper \cite{levy2015improving}, these hyper-parameters also affect performance of a trained model. Therefore in order to shorten the training time while preserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyper-parameters in the most performing way while optimizing the training process. Several approaches are to be introduced and their performance as well as training time will be used to compare with vanilla base-line algorithm. But at first we'll go through some existed approaches.

The work from Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. In the conventional distributed training system mentioned in that paper, word vectors are divided column-wise. Each part of every vector is saved in a dedicated parameter server, so called PS shard. And several word2vec clients do the training orchestration by accessing corpus on distributed file system and distributing the selected context, central words as well as negative samplings to PS shards. The word2vec clients also combine the partial vectors received from PS shards and send update or alternative partial vectors to PS shards using \verb|get| and \verb|put| requests. The transfer of output and input word vectors from PS shards to word2vec clients, as well as gradient from word2vec clients to PS Shards provoke huge network throughput. The solution to that mentioned in \citep{ordentlich2016network} is using remote procedure call (RPC) to inform every PS Shard to calculate dot product and update gradient locally, instead of require embedding vectors from PS Shard and send gradient back. This decrease the network throughput significantly while maintain the performance of trained model.

Another paper from Ji, Shihao et. al. \cite{ji2016parallelizing} provides another view of optimization word2vec training process. They focus on the initial sequentiality in Stochastic Gradient Descent (SGD) process. fan optimized scheme of leveraging BLAS level 3 function to accelerate training process. Since the work from Feng Niu et. al. \cite{recht2011hogwild} proves that the gradient update in SGD may proceed in parallel without concerning about race condition between different update of vectors at the same time, when the corpus is sparse. However, even if the training process leverages Hogwild! \cite{recht2011hogwild} lock-free strategy to accelerate SGD process, in each iteration only one pair of context word - central word (with negative samples) are used for training. Which means, regarding the embedded vectors, only one vector is to update. Which means level-1 BLAS operations only can be used for acceleration. This is limited by memory bandwidth. The work of paper propose that all the context words range over in each window can be processed in one iteration using same target(central) word and negative samples. The propagation is now a multiplication between two matrices and BLAS level-3 operations could be leveraged.

However, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an efficient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the conflict and increment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierarchical softmax can benefit due to its tree-type structure and  thus frequent access of the root.
