\section{Speed-up Approaches so far}
Training a well-performed natural language model entails huge size of corpora. However, this training process is highly time consuming. For example, the experiment with vanilla configuration mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be conducted. Hyper-parameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count-of-word when building vocabulary from whole corpus may do effect on training time. Also according to the paper \cite{levy2015improving}, these hyper-parameters also affect performance of a trained model. Therefore in order to shorten the training time while preserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyper-parameters in the most performing way while optimizing the training process. Our work here focus on the later part of acceleration and keep hyper-parameters the same with the most performing configuration as \cite{levy2015improving}. Several approaches are to be introduced and their performance as well as training time will be used to compare with vanilla base-line algorithm. But at first we'll go through some existed approaches to find out what's new in our method.

The work from Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. In the conventional distributed training system mentioned in that paper, word vectors are divided column-wise. Each part of every vector is saved in a dedicated parameter server, so called PS shard. And several word2vec clients do the training orchestration by accessing corpus on distributed file system and distributing the selected context, central words as well as negative samplings to PS shards. The word2vec clients also combine the partial vectors received from PS shards and send update or alternative partial vectors to PS shards using \verb|get| and \verb|put| requests. The transfer of output and input word vectors from PS shards to word2vec clients, as well as gradient from word2vec clients to PS Shards provoke huge network throughput. The solution to that mentioned in \citep{ordentlich2016network} is using remote procedure call (RPC) to inform every PS Shard to calculate dot product and update gradient locally, instead of require embedding vectors from PS Shard and send gradient back. This decrease the network throughput significantly while maintain the performance of trained model.

Another paper from Ji, Shihao et. al. \cite{ji2016parallelizing} provides another view of optimization word2vec training process. They focus on the initial sequentiality in Stochastic Gradient Descent (SGD) process. fan optimized scheme of leveraging BLAS level 3 function to accelerate training process. Since the work from Feng Niu et. al. \cite{recht2011hogwild} proves that the gradient update in SGD may proceed in parallel without concerning about race condition between different update of vectors at the same time, when the corpus is sparse. However, even if the training process leverages Hogwild! \cite{recht2011hogwild} lock-free strategy to accelerate SGD process, in each iteration only one pair of context word - central word (with negative samples) are used for training. Which means, regarding the embedded vectors, only one vector is to update. Thus only level-1 BLAS operations can be used for acceleration. This is limited by memory bandwidth. The work from Ji et. al. propose that all the context words range over in each window can be processed in one iteration using same target(central) word and negative samples. The propagation is now a multiplication between two matrices and BLAS level-3 operations could be leveraged.

In contrast to diminishing concurrent access to same memory when shared memory while using negative sampling presented by Ji et. al., the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} proposes an efficient way of using high-speed cache of CPU while using softmax on model training. Because of the frequent memory conflicts at root node of the tree in every word's path are effected by the memory conflict more significantly. They decided to make good use of CPU cache to turn global access of shared memory to local cache access.

Articles above concentrate basic on optimizing SGD and adapt it to a symmetric multi processing model. The work from Kaji and Kobayashi \cite{kaji2017incremental} proposes a on-line training method to exert all the potential of SGD as well as provide an approach of scalable training of word embedding. Comparing with the method proposed by Mikolov \cite{mikolov2013distributed} of building vocabulary distribution by scanning the corpus along side with building progress, so called \emph{batch SGNS}, Kaji et. al find their way out constructing vocabulary distribution and negative sampling increasingly with the help of a reservoir-based algorithm, named as \emph{incremental SGNS}. They also adapt the constant learning rate with AdaGrad to  increasing data mount for constructing words distribution. They have also proven theoretically that the expectation of their final result of incremental SGNS converges in probability to that of batch SGNS.