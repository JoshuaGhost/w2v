\section{Dedicate sub-models training and combination}
Inspired by the articles above, we in this article, however, present a framework of scalable training single word2vec model. Our framework not only leverages the acceleration of multiprocessing, but also is proven incrementally scalable. The training is divided into three phases. Division of corpus into fragments (\emph{partial corpus}), training the sub-models based on partial corpora and finally combining the sub-models together. This scaling framework of training word2vec model to present is fit for Map-Reduce \cite{dean2008mapreduce} and can be easily accelerated with a Hadoop cluster. The basic idea of this framework is that the training time and 'accuracy' of a distributed representation model depends substantially on size of corpus. All of word vectors form an ambient Euclidean space, a distributed word representation is a low-dimensional representation of one of words' subspaces \cite{Mahadevan2015reasoning}. Several "coarse" sub-models can be trained by using information from different partial corpora. Which can be divided so that the distribution of words are preserved. Then these sub-models can be combined to reduce the "coarseness". There are various of strategies can be performed while dividing corpus, aligning sub-models and combining them together. In the following subsections we will go through them one by one.

\input{s4/subsection1.tex}
\input{s4/subsection2.tex}
\input{s4/subsection3.tex}
\input{s4/subsection4.tex}
\input{s4/subsection5.tex}
