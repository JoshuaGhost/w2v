%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Accelerating Word2Vec training by leveraging parrallel computation of sub-models}

\author{Zijian Zhang \\
  Master of Science\\
  Informatik\\
  Leibniz Universit{\"a}t Hannover\\
  {\tt zhangzijian0523@gmail.com}\\ \And
  Avishek Anand\\
  Post Doctor\\
  L3S Research Center\\
  KBS Leibniz Universit{\"a}t Hannover\\
  {\tt test@dummy.com}\\
  }

\date{}

\begin{document}

\maketitle

\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between finishing training fast and performancing.
 
\end{abstract}


\section{Introduction}
In NLP applications, the first problem to solve is to find a propose representation (or token) of words \cite{schutze2008introduction}. One can use a random generated integer token, one-hot encoding \cite{turian2010word} or Haffman code \cite{el2006compression}, with respect to data compression, deal with it, as long as the representation could be \'understood\' by machines. However, representations such as simple integer don't take the semantic information into consideration \cite{le2014distributed}. While method such as one-hot encoding will also cause \textit{curse of dimentionality} if being directly used in an NLP applications \cite{bengio2003neural}. Word representations with respect of semantic and within low dimension are therefore need to be developed. 

As human beings, we can understand the meaning of one word through looking for the corresponded entry in a dictionary and read the discription. In order to understand the discription we have to find a discription of the discription, i.e. meta-discription, meta-meta-discription and so on all the way run into the akward stymie of self-reference. It's alright for a human brain but a catestrophy for modern computers who are based on formal arithmetic system. This self-reference can't be complete and conflict-free according to G{\"o"}del's incompleteness theorems \cite{godel1931formal}.

Thanks for the contribution by Zellig Harris, a basic hypothesis on distributional semantics was introducd that liguistic items with similar distributions have simillar meanings. Meaning, or semantic similarity between two linguistic expression depends strongly on the circumstance or the context they appear \cite{harris1954distributional}. Fortunatly, computers are champions in forming distributional properties by counting and regression.

This word representation dates back to 1986 due to Rumelhart, Hinton and Williams \cite{williams1986learning}. Word embedding uses co-occurrence of words and a softmax functional with help of stochastic gradient descent(SGD) to train a model, where each word is represented by, saying \'embedded in\' a vector in a high-dimensional space. In which model metric of distance (e.g. L2-Norm) between a pair of synonyms, or other pairs of words representing similiar concept is smaller than other pairs. Also those representation vectors of words have also \'parallelicity\' in word pairs, such that vec("Beijing")-vec("China")+vec("Germany") is close to vec("Berlin") \cite{le2014distributed}, which means vector from "China" to "Beijing" is somehow has the same direction and norm as the vector from "Germany" to "Berlin". Popular applications of word embedding are for example machine translation \cite{cho2014learning}, image annotation \cite{weston2011wsabie} and so on. 

Distributed word representation can be built through Point-wise Mutual Information (PMI) \cite{church1990word}. Moreover, it was later on researched that introduction of Positive PMI (PPMI) \cite{bullinaria2007extracting} and Shifted PMI (SPPMI, first presented in \cite{goldberg2014word2vec}) improves performance of models, considering the the fact that introduction of hyperparameter \verb|k| will shift the to optimise \verb|PMI(w, c)|, where \verb|k| is the number of negtive samples in Skip-gram with Negative Sampling and \verb|w| is the central word and \verb|c| is the nagtive sample or context word \cite{levy2014neural}. Methods such as PPMI and SVD are usually refered as \emph{"count based"} methods \cite{levy2015improving}, which focus on counting the coocurrence of word and it's context as well as performing linear transformation of the coocurrence matrix using techniques like lower-rank representation with help of SVD. To dig deeper with respect of co-occurrence matrix, GloVe \cite{Pennington2014glove} introduces a loss function

\begin{equation}
J=\sum_{i,j=1}^{V}f(X_ij)(w_i^T\tilde{w_j}+b_i\tilde{b_j}-logX_ij)^2,
\end{equation}

where $f(X_ij)$ is

\[f(x)=\begin{cases}
    (\frac{x}{x_{max}})^\alpha : \text{if }x<x_{max}\\
    1 : \text{otherwise}
    \end{cases}
\]

Alternatively, vector-based models can be generated with \emph{predictive} methods, which generally outperform the count-based methods \cite{levy2015improving}. The most notable of which is Skip-gram with Negative Sampling (SGNS) with neural network referred in \cite{mikolov2013distributed}, which uses a neural network with only one hidden-layer to generate embeddings. Which implicitly factorizes a shifted PMI matrix \cite{levy2014neural}. In this paper, the base-line model is generalized using framework \verb|gensim|\footnote{http://radimrehurek.com/gensim/} with SGNS and stochastic gradient descent (SGD) to minimize a loss function that defined on both co-occurence of words in a same window (positive co-occurence of central word and context) and negative co-occurrence between central word of a window and negtive sample words. 

\section{Brief introduction on word embedding with Skip-gram model}
As illustrated in \cite{mikolov2013distributed}, the goal of training process of a distributed representation model is to find a participate parameter combination for a 1-(hidden-)layer neural network. In this section we will go through how a distributed representation model of words as vectors is trained.

  \subsection{Notions and Conventions}
  \verb|V|: Set of all vocabularies. Before training the model using corpus, we need to identify all of the valible words, i.e. words do have meaning and are frequent. Normally a threshold $\tau_v$ is introduced, so that all the words occur less than $\tau_v$ times within whole corpus are pruned. For the sake of a systematic comparition, in our experiment we use vocabulary extracted from \verb|GoogleNews-vectors-negative300.bin|\footnote{found in https://code.google.com/archive/p/word2vec/}, whoes count is approximately same as \cite{levy2015improving}.

  \verb|v|: Number of vocabulary, i.e. $|V|$.

  \verb|X|: Input model of the neural network, A matrix with shape of $1 \times v$ .

This input is represented by matrix $X$ with shape of $1\times v$, where $v$ is size of vocabulary. As the same name in \verb|gensim|, we now call the hidden layer \verb|syn0|, i.e. vectors in the embedded space, a matrix with shape of $v\times dim$ where $dim$ is the number of dimentions of embedded space. While the connection matrix between hidden layer and output layer is called \verb|W| or \verb|syn1neg|, also in consistence with in \verb|gensim|, and which is a $(n+1)\times dim$ matrix, $n$ is the number of negtive samples, the $+1$ indicates the target word. With Skip-gram method in \cite{mikolov2013distributed} using a simplified variant of Noise Contrastive Estimation (NCE) \cite{gutmann2012noise}, the activation function in output layer is logit function: $y=\frac{1}{1+\exp(-syn0\times W^T)}$. In each round of training, we at first find the context word and target word in the same window, then after forward propagation with respect to network structure, the activation function calculates predicted labels, which can be treated as 'cooccurrence' between context word and target word, as well as 'cooccurrence' between negtive sampled words and target word. Then the error are defined as difference between labels (context word = 1, negtive sampled word = 0) and predicted result, which is used to multiplicate with learning factor $\alpha$ in the backward propagation. These iterate until the result convergente.

\section{System-level Optimization Methods}
Training a well-performanced natural language model requires huge size of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be performed. Hyperparameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyperparameters also affect performance of a trained model. Therefore in order to shorten the training time while perserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyperparameters in the most performansing way then parallelize the training process. Several methods are later on to be introduced and used to compair training time as well as performance with base-line model.

There are already several ways of parallel computation models of words embedding. For example the work of Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. The transfering of both output and input vectors of words to the word2vec client, as well as gradient to PS Shards, provoke huge network thoughput. \citep{ordentlich2016network}'s solution to that is divide each of embedding vectors "vertically (divide a d-dimention vector into k d/k dimention vectors)" into several components, each of which is maintained by one of the PS Shards. Everytime after each word2vec client select a central word with its context as well as negtive samplings (e.g. in Skip-gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, avoiding transfering of intermediate result and reduce the training time.

Another paper from Ji, Shao et. al. \cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild \cite{recht2011hogwild} log-free strategy. Everytime after a central word, its context word and negtive samplings was selected, a dot pruduct between embedding of context word and input weight of target(central) word and negtive samples have to be calculated. in legacy Hogwild, negtive samples and target words could be mutually different among different contexts. Under this circumstance the scala product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negtive samples. The scala product is the between two matrixes and BLAS level 3 functions could dedicate.

Contrastly, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an effecient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the confilit and increasment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierachical softmax can benifit due to its tree-type structure and  thus frequent access of the root.

\section{Dedicate sub-models training and combination}
Unlike the systematic way mentioned in the section above, we present in this paper essentially such method that at first train dedicate sub-models separately, each of wich consume part of corpus file, instead of the whole file. These processes can be run at different processors or even different mappers in a MapReduce cluster \cite{dean2008mapreduce}. Then these sub-models are combined with several different strategies. Due to \verb|iter| times passing-through of whole corpus in the traditional training process, it's trivial that the training time of a distributed representation model depends substaintially on size of corpus. All of word vectors form an ambient Euclidean space, the purpose of distributed word representation is construction of a low-dimensional representation of one of its subspace \cite{Mahadevan2015reasoning}. Several "coarse" sub-models can be trained by using information from different parts of corpus. Which can be irrelevant with each other or be sampled so that the distribution of words are preserved. Then these sub-models can be combined using different strategies to reduce the "coarseness" of each single one of them. There are various of strategies can be conducted while dividing corpus, aligning of manifolds in each models and combining them together. In the following subsections we will go through them one by one.

  \subsection{Dividing corpus}
  We can divide the corpora using a interweaving way. At first we denote sentences in all of documents in corpus with integers and if we divide the whole corpus into k pieces, the t-th piece becomes all centences that denoted as $kn+(t-1)$ (where $k\in Z_+$ and $t<k$). For example if we have the sentences numbered as 0, 1, 2, 3, 4, 5, ... in the 0th piece becomes 0, 2, 4, ... while the 1st becomes 1, 3, 5, ... Division using this way enables each piece contains different sentences, while spread each documentation into different pieces by granularity of sentence.
  If maintenance of word distribution is required for the sake of fairness, sampling methods like Bootstrap method \cite{efron1992bootstrap} can be leveraged. Still at first all of sentences contained in corpus are denoted by integers. Then sample k pieces {\`a} M/k centences using Bootstrap method, in order to construct k sub-corpus.
  \subsection{Alignment between models}
  After training sub-model using each sub-corpus dedicately, we can proceed to the combination phase of the whole work flow. One simple approach of combination is to just add different embedded vectors together, hoping the local information presented by a single model can also be maintained using single vector addition without any alignment. But inspecting only two word vector pairs from model 0 and model 1, say $(\vec{a}_0, \vec{b}_0)$ and $(\vec{a}_1, \vec{b}_1)$, without losing the generality. The inner product of each word vectors pair illustrates later on the similarity of word \verb|a| and \verb|b| in each model respectively. The simple vector addition of vectors from model 0 and model 1 dosen't guarantee the information expressed by inner product still preserves. We expect that in the merged model, inner product of $(\vec{a}_m,\vec{b}_m)$ should be a function that depends on only $\langle\vec{a}_0,\vec{b}_0\rangle$ and $\langle\vec{a}_1, \vec{b}_1\rangle$. However after using the simple vector addition, the new inner product is:
  \begin{equation}\label{eq:inner_product}
  \begin{split}
  &\langle(\vec{a}_0+\vec{a}_1), (\vec{b}_0+\vec{b}_1)\rangle\\
  = &\langle\vec{a}_0,\vec{b}_0\rangle+
    \langle\vec{a}_1,\vec{b}_1\rangle+
    \langle\vec{a}_0,\vec{b}_1\rangle+
    \langle\vec{a}_1,\vec{b}_0\rangle
  \end{split}
  \end{equation}
  where the last two part in the right side dependes on the inter-model-distortion because of the unalignment of models. Thus some alignment should be performed.

  \subsubsection{Alignment through Othogonal Linear Transformation}
  Defined and solved in \cite{schonemann1966generalized}, the Orthogonal Procrustus Problem focuses on solve such problem: Given matrix \verb|A| and matrix \verb|B|, find a orthogonal tansformation matrix T so that the squared mean error (SME) between transformed A using T and B is minimized. Mathematically speaking, define
  \begin{equation}
  T = \min_{T}tr[E^\intercal E]
  \end{equation}
  where 
  \begin{equation}
  E=B-AT
  \end{equation}
  under the constraint that 
  \begin{equation}
  TT^\intercal=T^\intercal T=I
  \end{equation}
  This transformation minimize the inter-model distortion under the constraint that only orthogonal transformations are allowd. Thus reduce the last two items in the \eqref{eq:inner_product} in a way.

  \subsubsection{Low rank alignment}
  The work from Boucher et al. \cite{boucher2015aligning} provides another approach of aligning different manifold together. Consider \verb|X| and \verb|Y| are two manifold to be aligned. They are at first decomposed using SVD such that $X=U_xS_xV_x^\intercal$ and $Y=U_yS_yV_y^\intercal$. With out losing the generality, $S_x$ and $V_x$ are artitioned into $V_x=[V_{x1}V_{x2}]$ and $S_x=[S_{x1}S_{x2}]$ according to 
  $$
  I_1=\{i:s_i>1 \forall s_i \in S\}
  $$
  and
  $$
  I_2=\{i:s_i\leq 1 \forall s_i\in S\}.
  $$ 
  This decomposition is used for preparation of low-rank-representation X and Y using Low rank embedding (LRE). The LRE problem is defined as that, given a data set X, finding a proper transformation matrix R, in order to minimize the loss function,

  \begin{equation}\label{eq:LRE_objection}
  min_R\frac{1}{2}\|X-XR\|^2_F+\lambda\|R\|_*,
  \end{equation}

  where $\lambda > 0, \|X\|_F=\sqrt{\sum_i\sum_j|x_{i,j}|^2}$ is called Frobenius norm, while $\|X\|_*=\sum_i\sigma_i(X)$ is the spectral norm and where by $\sigma_i$ are singular values. \cite{candes2010power} proved that the formula \eqref{eq:LRE_objection} is a convex relaxation of rank minimization problem and it's result \verb|R| is the so-called reconstruction coefficients, which discribe the intra-manifold relationship between points inside a single manifold. The \eqref{eq:LRE_objection} is showed in \cite{favaro2011closed} can be solved in closed form. This is where we use the decomposition of \verb|X| and \verb|Y|. For example, the optimal closed-form solution of reconstruction of coefficients for \verb|X| is
  
  \begin{equation}
  R^{(X)}=V_x1(I-S_x1^{-1})V_x1^\intercal.
  \end{equation}

  Once the $R^{(X)}$ and $R^{(Y)}$ are calculated, they can be blocked as
  
  \[R= \begin{bmatrix}
      R^{(X)} & 0 \\
      0 & R^{(Y)}
      \end{bmatrix}\] and 
  \[C=\begin{bmatrix}
      0 & C^{(X,Y)}\\
      C^{(Y,X)} & 0
      \end{bmatrix}\],

  where $C^{(X,Y)}$ is inter-manifolds correspondence, defined as

  \[C_{i,j}^{(X,Y)}=\begin{cases}
                    1 : X_i\text{ is in correspondence with }Y_i\\
                    0 : \text{otherwise}
                    \end{cases}\].

  Defining $F\in {\rm I\!R}^{(2N\times d)}$ as

  \[F=\begin{bmatrix}
      F^{(X)}\
      F^{(Y)}
      \end{bmatrix}\],

  where \verb|N| is the number of points in each manifold and \verb|d| is the dimension of both manifolds, $F^{(X)}$ and $F^{(Y)}$ are the aligned manifolds. The alignment precision of \verb|F| can be discribed as loss function

  \begin{equation}\label{eq:alignment_precision_objection}
  \mathcal{Z}(F)=(1-\mu )\|F-RF\|^2_F+\mu\sum_{i,j=1}^N\|F_i-F_j\|^2C_{i,j},
  \end{equation}

  where $\mu\in[0,1]$ is a hyper parameter that controls the inter-manifold correspondence or intra-manifold correspondence matters significantly. With help of the Lagrange multipliers method, equation \eqref{eq:alignment_precision_objection} can be solved by finding the d smallest non-zero eigenvectors of the matrix

  \begin{equation}
  (1-\mu)M+2\mu L,
  \end{equation}

  where

  \[M=\begin{bmatrix}
    (I-R^{(x)})^2 & 0\\
    0 & (i-R^{(Y)})^2
    \end{bmatrix}\],

  and

  \[L=\begin{bmatrix}
    D^X & -C^{(X,Y)}\\
    (-C^{(X,Y)})^\intercal & D^Y
    \end{bmatrix}\],

  where by \[D=\begin{bmatrix}D^X & 0\\0 & D^Y\end{bmatrix}\] is a diagonal matrix.
  \subsection{Normalized or Unnormalized?}
  According to work of Levy et al. \cite{levy2015improving}, 
  \subsection{Combination of sub-models}
  \subsubsection{with Vector Addition}
  \subsubsection{with PCA projection}
  \subsubsection{}
  \subsection{Combination using ensemble learning}

\section{Experiment result}
In this paper, we keep hyperparameters same as most-performed SGNS model evaluated in \cite{levy2015improving}, namely \verb|win=10|, \verb|neg=5|, \verb|dim=500|, \verb|iter=5|. English wiki dump (enwiki-latest-pages-articles on March 2nd, 2016\footnote{from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}) is used as corpus. In table \ref{table:benchmark} the line "base-line" representes the performance of model using whole corpus. each column stands for a evaluation dataset or training time. This benchmark is also based on \cite{levy2015improving}.
\begin{table*}
\caption{Performance and training time of different scalable strategies}
\begin{tabular}{c|cccccccc|c}
\hline
Dataset   & AP   & MEN  & MTurk & WS353 & WS353R & WS353S & Google & MSR  & training time\ \hline
Base-line & .595 & .736 & .694  & .611  & .514   & .754   & .661   & .440 & 5d 3h 11m 25s\\ \hline
\end{tabular}
\label{table:benchmark}
\end{table*}

Because  rely substentially on the hypothesis that each word in vocabulary is independently distributedthere is a be trained and matained is inspired by Ising model, which discription stochastic mechanic This is respired by the stochastic mechanic metaphor of distributive representation of words. Each corpus set could be treated as a thermal system 
\subsubsection{Direct combination on embedded vectors}
%%\subsubsubsection{Alignment of embedded chat}
%%\subsubsubsection{Form a new distribution}
\subsubsection{Training a new model using ensembled prediction}

\subsection{Optimazation using variance reduced stochastic gradient descent(VRSGD)}
\subsubsection{Similarity between SG-MCMC and stochastic optimization}
\subsubsection{Asynchronous stochastic gradient descent with variance reduction for non-convex optimization}
\subsubsection{Inspiration from SVRG}
\subsection{Experiment results}

\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
\grid
