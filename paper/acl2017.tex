%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Accelerating Word2Vec training by leveraging parrallel computation of sub-models}

\author{Zijian Zhang \\
  Master of Science\\
  Informatik\\
  Leibniz Universit{\"a}t Hannover\\
  {\tt zhangzijian0523@gmail.com}\\ \And
  Avishek Anand\\
  Post Doctor\\
  L3S Research Center\\
  KBS Leibniz Universit{\"a}t Hannover\\
  {\tt test@dummy.com}\\
  }

\date{}

\begin{document}

\maketitle

\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between finishing training fast and performancing.
 
\end{abstract}


\section{Introduction}
In NLP applications, the first problem to solve is to find a propose representation (or token) of words \cite{schutze2008introduction}. One can use a random generated integer token, one-hot encoding \cite{turian2010word} or Haffman code \cite{el2006compression}, with respect to data compression, deal with it, as long as the representation could be \'understood\' by machines. However, representations such as simple integer don't take the semantic information into consideration \cite{le2014distributed}. While method such as one-hot encoding will also cause \textit{curse of dimentionality} if being directly used in an NLP applications \cite{bengio2003neural}. Word representations with respect of semantic and within low dimension are therefore need to be developed. 

As human beings, we can understand the meaning of one word through looking for the corresponded entry in a dictionary and read the discription. In order to understand the discription we have to find a discription of the discription, i.e. meta-discription, meta-meta-discription and so on all the way run into the akward stymie of self-reference. It's alright for a human brain but a catestrophy for modern computers who are based on formal arithmetic system. This self-reference can't be complete and conflict-free according to G{\"o"}del's incompleteness theorems \cite{godel1931formal}.

Thanks for the contribution by Zellig Harris, a basic hypothesis on distributional semantics was introducd that liguistic items with similar distributions have simillar meanings. Meaning, or semantic similarity between two linguistic expression depends strongly on the circumstance or the context they appear \cite{harris1954distributional}. Fortunatly, computers are champions in forming distributional properties by counting and regression.

This word representation dates back to 1986 due to Rumelhart, Hinton and Williams \cite{williams1986learning}. Word embedding uses co-occurrence of words and a softmax functional with help of stochastic gradient descent(SGD) to train a model, where each word is represented by, saying \'embedded in\' a vector in a high-dimensional space. In which model metric of distance (e.g. L2-Norm) between a pair of synonyms, or other pairs of words representing similiar concept is smaller than other pairs. Also those representation vectors of words have also \'parallelicity\' in word pairs, such that vec("Beijing")-vec("China")+vec("Germany") is close to vec("Berlin") \cite{le2014distributed}, which means vector from "China" to "Beijing" is somehow has the same direction and norm as the vector from "Germany" to "Berlin". Popular applications of word embedding are for example machine translation \cite{cho2014learning}, image annotation \cite{weston2011wsabie} and so on. 

Distributed word representation can be built through Point-wise Mutual Information (PMI) \cite{church1990word}. Moreover, it was later on researched that introduction of Positive PMI (PPMI) \cite{bullinaria2007extracting} and Shifted PMI (SPPMI, first presented in \cite{goldberg2014word2vec}) improves performance of models, considering the the fact that introduction of hyperparameter \verb|k| will shift the to optimise \verb|PMI(w, c)|, where \verb|k| is the number of negtive samples in Skip-gram with Negative Sampling and \verb|w| is the central word and \verb|c| is the nagtive sample or context word \cite{levy2014neural}. Methods such as PPMI and SVD are usually refered as \emph{"count based"} methods \cite{levy2015improving}, which focus on counting the coocurrence of word and it's context as well as performing linear transformation of the coocurrence matrix using techniques like lower-rank representation with help of SVD. To dig deeper with respect of co-occurrence matrix, GloVe \cite{Pennington2014glove} introduces a loss function:
$$J=\sum_{i,j=1}^{V}f(X_ij)(w_i^T\tilde{w_j}+b_i\tilde{b_j}-logX_ij)^2$$
Where $f(X_ij)$ is
$$ f(x)=\left\{
    \begin{array}{rl}
    (\frac{x}{x_{max}})^\alpha & if\quad x<x_{max}\\
    1 & otherwise
    \end{array}
\right.
$$
Alternatively, vector-based models can be generated with \emph{predictive} methods, which generally outperform the count-based methods \cite{levy2015improving}. The most notable of which is Skip-gram with Negative Sampling (SGNS) with neural network referred in \cite{mikolov2013distributed}, which uses a neural network with only one hidden-layer to generate embeddings. Which implicitly factorizes a shifted PMI matrix \cite{levy2014neural}. In this paper, the base-line model is generalized using framework \verb|gensim|\footnote{http://radimrehurek.com/gensim/} with SGNS and stochastic gradient descent (SGD) to minimize a loss function that defined on both co-occurence of words in a same window (positive co-occurence of central word and context) and negative co-occurrence between central word of a window and negtive sample words. 

\section{Brief introduction on word embedding with Skip-gram model}
As illustrated in \cite{mikolov2013distributed}, the goal of training process of a distributed representation model is to find a participate parameter combination for a 1-(hidden-)layer neural network. In this section we will go through how a distributed representation model of words as vectors is trained.

  \subsection{Notions and Conventions}
  \verb|V|: Set of all vocabularies. Before training the model using corpus, we need to identify all of the valible words, i.e. words do have meaning and are frequent. Normally a threshold $\tau_v$ is introduced, so that all the words occur less than $\tau_v$ times within whole corpus are pruned. For the sake of a systematic comparition, in our experiment we use vocabulary extracted from \verb|GoogleNews-vectors-negative300.bin|\footnote{found in https://code.google.com/archive/p/word2vec/}, whoes count is approximately same as \cite{levy2015improving}.

  \verb|v|: Number of vocabulary, i.e. $|V|$.

  \verb|X|: Input model of the neural network, A matrix with shape of $1 \times v$ .

This input is represented by matrix $X$ with shape of $1\times v$, where $v$ is size of vocabulary. As the same name in \verb|gensim|, we now call the hidden layer \verb|syn0|, i.e. vectors in the embedded space, a matrix with shape of $v\times dim$ where $dim$ is the number of dimentions of embedded space. While the connection matrix between hidden layer and output layer is called \verb|W| or \verb|syn1neg|, also in consistence with in \verb|gensim|, and which is a $(n+1)\times dim$ matrix, $n$ is the number of negtive samples, the $+1$ indicates the target word. With Skip-gram method in \cite{mikolov2013distributed} using a simplified variant of Noise Contrastive Estimation (NCE) \cite{gutmann2012noise}, the activation function in output layer is logit function: $y=\frac{1}{1+\exp(-syn0\times W^T)}$. In each round of training, we at first find the context word and target word in the same window, then after forward propagation with respect to network structure, the activation function calculates predicted labels, which can be treated as 'cooccurrence' between context word and target word, as well as 'cooccurrence' between negtive sampled words and target word. Then the error are defined as difference between labels (context word = 1, negtive sampled word = 0) and predicted result, which is used to multiplicate with learning factor $\alpha$ in the backward propagation. These iterate until the result convergente.

\section{System-level Optimization Methods}
Training a well-performanced natural language model requires huge size of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be performed. Hyperparameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyperparameters also affect performance of a trained model. Therefore in order to shorten the training time while perserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyperparameters in the most performansing way then parallelize the training process. Several methods are later on to be introduced and used to compair training time as well as performance with base-line model.

There are already several ways of parallel computation models of words embedding. For example the work of Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. The transfering of both output and input vectors of words to the word2vec client, as well as gradient to PS Shards, provoke huge network thoughput. \citep{ordentlich2016network}'s solution to that is divide each of embedding vectors "vertically (divide a d-dimention vector into k d/k dimention vectors)" into several components, each of which is maintained by one of the PS Shards. Everytime after each word2vec client select a central word with its context as well as negtive samplings (e.g. in Skip-gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, avoiding transfering of intermediate result and reduce the training time.

Another paper from Ji, Shao et. al. \cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild \cite{recht2011hogwild} log-free strategy. Everytime after a central word, its context word and negtive samplings was selected, a dot pruduct between embedding of context word and input weight of target(central) word and negtive samples have to be calculated. in legacy Hogwild, negtive samples and target words could be mutually different among different contexts. Under this circumstance the scala product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negtive samples. The scala product is the between two matrixes and BLAS level 3 functions could dedicate.

Contrastly, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an effecient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the confilit and increasment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierachical softmax can benifit due to its tree-type structure and  thus frequent access of the root.

\section{Dedicate sub-models training and combination}
Unlike the systematic way mentioned in the section above, we present in this paper essentially such method that at first train dedicate sub-models parallelly, each of wich consume part of corpus file, instead of the whole file, then these sub-models are combined with several different strategies. It's clearly to see that the training time of a distributed representation model depends substaintially on size of its corpus. Afterall the training process goes through the whole corpus \verb|iter| times. The distribution space of all the vectors of words provided in the corpora are embedded in an ambient Euclidean space, the purpose of distributed word representation is to construct a low-dimensional representation of one of its subspace \cite{Mahadevan2015reasoning}. Several coarse sub-models can be only trained by using information from different parts of corpus. Which are irrelevant with each other and thus parallelly trainable. Then these sub-models can be combined using different strategies to reduce the coarseness of each single one of them. There are various of strategies can be conducted while dividing corpus, aligning of manifolds in each models and combining them together. In the following subsections we will go through them one by one.

  \subsection{Dividing corpora}
  \subsection{Alignment of manifolds}
  \subsection{Combination of sub-models}
  \subsection{Combination using ensemble learning}

\section{Experiment result}
In this paper, we keep hyperparameters same as most-performed SGNS model evaluated in \cite{levy2015improving}, namely \verb|win=10|, \verb|neg=5|, \verb|dim=500|, \verb|iter=5|. English wiki dump (enwiki-latest-pages-articles on March 2nd, 2016\footnote{from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}) is used as corpus. In table \ref{table:benchmark} the line "base-line" representes the performance of model using whole corpus. each column stands for a evaluation dataset or training time. This benchmark is also based on \cite{levy2015improving}.
\begin{table*}
\caption{Performance and training time of different scalable strategies}
\begin{tabular}{c|cccccccc|c}
\hline
Method    & AP   & MEN  & MTurk & WS353 & WS353R & WS353S & Google & MSR  & training time\\ \hline
Base-line & .595 & .736 & .694  & .611  & .514   & .754   & .661   & .440 & \\ \hline
\end{tabular}
\label{table:benchmark}
\end{table*}

Because  rely substentially on the hypothesis that each word in vocabulary is independently distributedthere is a be trained and matained is inspired by Ising model, which discription stochastic mechanic This is respired by the stochastic mechanic metaphor of distributive representation of words. Each corpus set could be treated as a thermal system 
\subsubsection{Direct combination on embedded vectors}
%%\subsubsubsection{Alignment of embedded chat}
%%\subsubsubsection{Form a new distribution}
\subsubsection{Training a new model using ensembled prediction}

\subsection{Optimazation using variance reduced stochastic gradient descent(VRSGD)}
\subsubsection{Similarity between SG-MCMC and stochastic optimization}
\subsubsection{Asynchronous stochastic gradient descent with variance reduction for non-convex optimization}
\subsubsection{Inspiration from SVRG}
\subsection{Experiment results}

\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
