%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{url}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Accelerating Word2Vec training by leveraging parallel computation of sub-models}

\author{
  Zijian Zhang \\
  Master of Science\\
  Informatik\\
  Leibniz Universit{\"a}t Hannover\\
  {\tt zhangzijian0523@gmail.com}\\ \And
  Avishek Anand\\
  Post Doctor\\
  L3S Research Center\\
  KBS Leibniz Universit{\"a}t Hannover\\
  {\tt anand@l3s.de}\\
  }

\date{}

\begin{document}

\maketitle

\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between training speed and performance.
 
\end{abstract}


\section{Introduction}
In NLP applications, the first problem to solve is to find a propose representation (or token) of words \cite{schutze2008introduction}. One can use a random generated integer token, one-hot encoding \cite{turian2010word} or Huffman code \cite{el2006compression}, with respect to data compression, deal with it, as long as the representation could be \'understood\' by machines. However, representations such as simple integer don't take the semantic information into consideration \cite{le2014distributed}. While method such as one-hot encoding will also cause \textit{curse of dimensionality} if being directly used in an NLP applications \cite{bengio2003neural}. Word representations with respect of semantic and within low dimension are therefore need to be developed. 

As human beings, we can understand the meaning of one word through looking for the corresponded entry in a dictionary and read the description. In order to understand the description we have to find a description of the description, i.e. meta-description, meta-meta-description and so on all the way run into the awkward stymie of self-reference. It's alright for a human brain but a catastrophe for modern computers who are based on formal arithmetic system. This self-reference can't be complete and conflict-free according to G{\"o"}del's incompleteness theorems \cite{godel1931formal}.

Thanks for the contribution by Zellig Harris, a basic hypothesis on distributional semantics was introduced that linguistic items with similar distributions have similar meanings. Meaning, or semantic similarity between two linguistic expression depends strongly on the circumstance or the context they appear \cite{harris1954distributional}. Fortunately, computers are champions in forming distributional properties by counting and regression.

This word representation dates back to 1986 due to Rumelhart, Hinton and Williams \cite{williams1986learning}. Word embedding uses co-occurrence of words and a softmax functional with help of stochastic gradient descent(SGD) to train a model, where each word is represented by, saying \'embedded in\' a vector in a high-dimensional space. In which model metric of distance (e.g. L2-Norm) between a pair of synonyms, or other pairs of words representing similar concept is smaller than other pairs. Also those representation vectors of words have also \'parallelism\' in word pairs, such that vec("Beijing")-vec("China")+vec("Germany") is close to vec("Berlin") \cite{le2014distributed}, which means vector from "China" to "Beijing" is somehow has the same direction and norm as the vector from "Germany" to "Berlin". Popular applications of word embedding are for example machine translation \cite{cho2014learning}, image annotation \cite{weston2011wsabie} and so on. 

Distributed word representation can be built through Point-wise Mutual Information (PMI) \cite{church1990word}. Moreover, it was later on researched that introduction of Positive PMI (PPMI) \cite{bullinaria2007extracting} and Shifted PMI (SPPMI, first presented in \cite{goldberg2014word2vec}) improves performance of models, considering the the fact that introduction of hyper-parameter \verb|k| will shift the to optimize \verb|PMI(w, c)|, where \verb|k| is the number of negative samples in Skip-gram with Negative Sampling and \verb|w| is the central word and \verb|c| is the native sample or context word \cite{levy2014neural}. Methods such as PPMI and SVD are usually referred as \emph{"count based"} methods \cite{levy2015improving}, which focus on counting the co-occurrence of word and it's context as well as performing linear transformation of the co-occurrence matrix using techniques like lower-rank representation with help of SVD. To dig deeper with respect of co-occurrence matrix, GloVe \cite{Pennington2014glove} introduces a loss function

\begin{equation}
J=\sum_{i,j=1}^{V}f(X_ij)(w_i^T\tilde{w_j}+b_i\tilde{b_j}-logX_ij)^2,
\end{equation}

where $f(X_ij)$ is

\[f(x)=\begin{cases}
    (\frac{x}{x_{max}})^\alpha : \text{if }x<x_{max}\\
    1 : \text{otherwise}
    \end{cases}
\]

Alternatively, vector-based models can be generated with \emph{predictive} methods, which generally outperform the count-based methods \cite{levy2015improving}. The most notable of which is Skip-gram with Negative Sampling (SGNS) with neural network referred in \cite{mikolov2013distributed}, which uses a neural network with only one hidden-layer to generate embeddings. Which implicitly factorizes a shifted PMI matrix \cite{levy2014neural}. In this paper, the base-line model is generalized using framework \verb|gensim|\footnote{http://radimrehurek.com/gensim/} with SGNS and stochastic gradient descent (SGD) to minimize a loss function that defined on both co-occurrence of words in a same window (positive co-occurrence of central word and context) and negative co-occurrence between central word of a window and negative sample words. 

\section{Brief introduction on word embedding with Skip-gram model}
As illustrated in \cite{mikolov2013distributed}, the goal of training process of a distributed representation model is to find a participate parameter combination for a 1-(hidden-)layer neural network. In this section we will go through how a distributed representation model of words as vectors is trained.

  \subsection{Notions and Conventions}
  \verb|V|: Set of all vocabularies. Before training the model using corpus, we need to identify all of the legal words, i.e. words do have meaning and are frequent. Normally a threshold $\tau_v$ is introduced, so that all the words occur less than $\tau_v$ times within whole corpus are pruned. For the sake of a systematic comparison, in our experiment we use vocabulary extracted from \verb|GoogleNews-vectors-negative300.bin|\footnote{found in https://code.google.com/archive/p/word2vec/}, whose count is approximately same as \cite{levy2015improving}.

  \verb|v|: Number of vocabulary, i.e. $|V|$.

  \verb|X|: Input model of the neural network, A matrix with shape of $1 \times v$ .

This input is represented by matrix $X$ with shape of $1\times v$, where $v$ is size of vocabulary. As the same name in \verb|gensim|, we now call the hidden layer \verb|syn0|, i.e. vectors in the embedded space, a matrix with shape of $v\times dim$ where $dim$ is the number of dimensions of embedded space. While the connection matrix between hidden layer and output layer is called \verb|W| or \verb|syn1neg|, also in consistence with in \verb|gensim|, and which is a $(n+1)\times dim$ matrix, $n$ is the number of negtive samples, the $+1$ indicates the target word. With Skip-gram method in \cite{mikolov2013distributed} using a simplified variant of Noise Contrastive Estimation (NCE) \cite{gutmann2012noise}, the activation function in output layer is logit function: $y=\frac{1}{1+\exp(-syn0\times W^T)}$. In each round of training, we at first find the context word and target word in the same window, then after forward propagation with respect to network structure, the activation function calculates predicted labels, which can be treated as 'co-occurrence' between context word and target word, as well as 'co-occurrence' between negative sampled words and target word. Then the error are defined as difference between labels (context word = 1, negtive sampled word = 0) and predicted result, which is used to multiply with learning factor $\alpha$ in the backward propagation. These iterate until the result convergent.

\section{System-level Optimization Methods}
Training a well-performed natural language model requires huge size of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be performed. Hyper-parameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyper-parameters also affect performance of a trained model. Therefore in order to shorten the training time while preserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyper-parameters in the most performing way then parallelize the training process. Several methods are later on to be introduced and used to compare training time as well as performance with base-line model.

There are already several ways of parallel computation models of words embedding. For example the work of Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. The transferring of both output and input vectors of words to the word2vec client, as well as gradient to PS Shards, provoke huge network throughput. \citep{ordentlich2016network}'s solution to that is divide each of embedding vectors "vertically (divide a d-dimension vector into k d/k dimension vectors)" into several components, each of which is maintained by one of the PS Shards. Every time after each word2vec client select a central word with its context as well as negative samplings (e.g. in Skip-gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, avoiding transferring of intermediate result and reduce the training time.

Another paper from Ji, Shao et. al. \cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild \cite{recht2011hogwild} log-free strategy. Every time after a central word, its context word and negative samplings was selected, a dot product between embedding of context word and input weight of target(central) word and negative samples have to be calculated. in legacy Hogwild, negative samples and target words could be mutually different among different contexts. Under this circumstance the scalar product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negative samples. The scalar product is the between two matrices and BLAS level 3 functions could dedicate.

Contrastively, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an efficient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the conflict and increment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierarchical softmax can benefit due to its tree-type structure and  thus frequent access of the root.

\section{Dedicate sub-models training and combination}
Unlike the systematic way mentioned in the section above, we present in this paper essentially such method that at first train dedicate sub-models separately, each of which consume part of corpus file, instead of the whole file. These processes can be run at different processors or even different mappers in a MapReduce cluster \cite{dean2008mapreduce}. Then these sub-models are combined with several different strategies. Due to \verb|iter| times passing-through of whole corpus in the traditional training process, it's trivial that the training time of a distributed representation model depends substantially on size of corpus. All of word vectors form an ambient Euclidean space, the purpose of distributed word representation is construction of a low-dimensional representation of one of its subspace \cite{Mahadevan2015reasoning}. Several "coarse" sub-models can be trained by using information from different parts of corpus. Which can be irrelevant with each other or be sampled so that the distribution of words are preserved. Then these sub-models can be combined using different strategies to reduce the "coarseness" of each single one of them. There are various of strategies can be conducted while dividing corpus, aligning of manifolds in each models and combining them together. In the following subsections we will go through them one by one.

  \subsection{Dividing corpus}
  We can divide the corpora using a interweaving way. At first we denote sentences in all of documents in corpus with integers and if we divide the whole corpus into k pieces, the t-th piece becomes all sentences that denoted as $kn+(t-1)$ (where $k\in Z_+$ and $t<k$). For example if we have the sentences numbered as 0, 1, 2, 3, 4, 5, ... in the 0th piece becomes 0, 2, 4, ... while the 1st becomes 1, 3, 5, ... Division using this way enables each piece contains different sentences, while spread each documentation into different pieces by granularity of sentence.
  If maintenance of word distribution is required for the sake of fairness, sampling methods like Bootstrap method \cite{efron1992bootstrap} can be leveraged. Still at first all of sentences contained in corpus are denoted by integers. Then sample k pieces {\`a} M/k centences using Bootstrap method, in order to construct k sub-corpus.
  
  \subsection{Order while combining models}
  \subsubsection{Sequential order}
  \subsubsection{Dichotomize order}
  \subsubsection{Combine two with minimum Procrustes error}

  \subsection{Alignment between models}
  After training sub-model using each sub-corpus dedicatedly, we can proceed to the combination phase of the whole work flow. One simple approach of combination is to just add different embedded vectors together, hoping the local information presented by a single model can also be maintained using single vector addition without any alignment. But inspecting only two word vector pairs from model 0 and model 1, say $(\vec{a}_0, \vec{b}_0)$ and $(\vec{a}_1, \vec{b}_1)$, without losing the generality. The inner product of each vector pair illustrates the similarity of word \verb|a| and \verb|b| in each model respectively. Simply adding vectors from model 0 with correspondent vectors in model 1 separately doesn't guarantee that information expressed by two dedicated models using inner product still preserves. We expect that in the merged model, inner product of $(\vec{a}_m,\vec{b}_m)$ should be a function that depends on only $\langle\vec{a}_0,\vec{b}_0\rangle$ and $\langle\vec{a}_1, \vec{b}_1\rangle$. However after using the simple vector addition, the new inner product is:
  \begin{equation}\label{eq:inner_product}
  \begin{split}
  &\langle(\vec{a}_0+\vec{a}_1), (\vec{b}_0+\vec{b}_1)\rangle\\
  = &\langle\vec{a}_0,\vec{b}_0\rangle+
    \langle\vec{a}_1,\vec{b}_1\rangle+
    \langle\vec{a}_0,\vec{b}_1\rangle+
    \langle\vec{a}_1,\vec{b}_0\rangle
  \end{split}
  \end{equation}
  where the last two part in the right side depends on the between-model-distortion because of the nonalignment of models. Thus some alignment should be performed.

  \subsubsection{Alignment through Orthogonal Linear Transformation}
  Defined and solved in \cite{schonemann1966generalized}, the Orthogonal Procrustes Problem focuses on solve such problem: Given matrix \verb|A| and matrix \verb|B|, find a orthogonal transformation matrix T so that the squared mean error (SME) between transformed A using T and B is minimized. Mathematically speaking, define
  \begin{equation}
  T = \min_{T}tr[E^\intercal E]
  \end{equation}
  where 
  \begin{equation}
  E=B-AT
  \end{equation}
  under the constraint that 
  \begin{equation}
  TT^\intercal=T^\intercal T=I
  \end{equation}
  This transformation minimize the inter-model distortion under the constraint that only orthogonal transformations are allowed. Thus reduce the last two items in the \eqref{eq:inner_product} in a way.

  \subsubsection{Low rank alignment}
  The work from Boucher et al. \cite{boucher2015aligning} provides another approach of aligning different manifold together. Consider \verb|X| and \verb|Y| are two manifold to be aligned. They are at first decomposed using SVD such that $X=U_xS_xV_x^\intercal$ and $Y=U_yS_yV_y^\intercal$. With out losing the generality, $S_x$ and $V_x$ are partitioned into $V_x=[V_{x1}V_{x2}]$ and $S_x=[S_{x1}S_{x2}]$ according to 
  $$
  I_1=\{i:s_i>1 \forall s_i \in S\}
  $$
  and
  $$
  I_2=\{i:s_i\leq 1 \forall s_i\in S\}.
  $$ 
  This decomposition is used for preparation of low-rank-representation X and Y using Low rank embedding (LRE). The LRE problem is defined as that, given a data set X, finding a proper transformation matrix R, in order to minimize the loss function,

  \begin{equation}\label{eq:LRE_objection}
  min_R\frac{1}{2}\|X-XR\|^2_F+\lambda\|R\|_*,
  \end{equation}

  where $\lambda > 0, \|X\|_F=\sqrt{\sum_i\sum_j|x_{i,j}|^2}$ is called Frobenius norm, while $\|X\|_*=\sum_i\sigma_i(X)$ is the spectral norm and where by $\sigma_i$ are singular values. \cite{candes2010power} proved that the formula \eqref{eq:LRE_objection} is a convex relaxation of rank minimization problem and it's result \verb|R| is the so-called reconstruction coefficients, which describe the intra-manifold relationship between points inside a single manifold. The \eqref{eq:LRE_objection} is showed in \cite{favaro2011closed} can be solved in closed form. This is where we use the decomposition of \verb|X| and \verb|Y|. For example, the optimal closed-form solution of reconstruction of coefficients for \verb|X| is
  
  \begin{equation}
  R^{(X)}=V_x1(I-S_x1^{-1})V_x1^\intercal.
  \end{equation}

  Once the $R^{(X)}$ and $R^{(Y)}$ are calculated, they can be blocked as
  
  \[R= \begin{bmatrix}
      R^{(X)} & 0 \\
      0 & R^{(Y)}
      \end{bmatrix}\] and 
  \[C=\begin{bmatrix}
      0 & C^{(X,Y)}\\
      C^{(Y,X)} & 0
      \end{bmatrix}\],

  where $C^{(X,Y)}$ is inter-manifolds correspondence, defined as

  \[C_{i,j}^{(X,Y)}=\begin{cases}
                    1 : X_i\text{ is in correspondence with }Y_i\\
                    0 : \text{otherwise}
                    \end{cases}\].

  Defining $F\in {\rm I\!R}^{(2N\times d)}$ as

  \[F=\begin{bmatrix}
      F^{(X)}\
      F^{(Y)}
      \end{bmatrix}\],

  where \verb|N| is the number of points in each manifold and \verb|d| is the dimension of both manifolds, $F^{(X)}$ and $F^{(Y)}$ are the aligned manifolds. The alignment precision of \verb|F| can be described as loss function

  \begin{equation}\label{eq:alignment_precision_objection}
  \mathcal{Z}(F)=(1-\mu )\|F-RF\|^2_F+\mu\sum_{i,j=1}^N\|F_i-F_j\|^2C_{i,j},
  \end{equation}

  where $\mu\in[0,1]$ is a hyper parameter that controls the inter-manifold correspondence or intra-manifold correspondence matters significantly. With help of the Lagrange multipliers method, equation \eqref{eq:alignment_precision_objection} can be solved by finding the d smallest non-zero eigenvectors of the matrix

  \begin{equation}
  (1-\mu)M+2\mu L,
  \end{equation}

  where

  \[M=\begin{bmatrix}
    (I-R^{(x)})^2 & 0\\
    0 & (i-R^{(Y)})^2
    \end{bmatrix}\],

  and

  \[L=\begin{bmatrix}
    D^X & -C^{(X,Y)}\\
    (-C^{(X,Y)})^\intercal & D^Y
    \end{bmatrix}\],

  where by \[D=\begin{bmatrix}D^X & 0\\0 & D^Y\end{bmatrix}\] is a diagonal matrix.
  \subsection{Normalized or Unnormalized?}
  According to work of Levy et al. \cite{levy2015improving}, normalization of each vector enables that every time when we calculate inner product of two normalized vectors, we are actually calculate the cosine similarity between them. Considering float number of dimensions composing original (unnormalized) vectors in each sub-model can scale differently, a normalization that every vector is kept to length 1 also makes it possible that when an orthogonal transformation-vector addition of two models is employed, the result will not trend to the model, whose vectors has higher L2-norm.

  \subsection{Combination of sub-models}
  After alignment (or without it) and normalization (or without it), it's finally the time of combination. In our experiments, two kinds of combination are employed, thus direct vector addition and projection using PCA. In following sections they are to be briefly depicted how and why they might or might not work in our experiments.

  \subsubsection{Using vector addition}
  Add vectors together is one of the first intuition when talking about combination two bundle of vectors. The direct addition of two vectors from two sub-models can be seen as calculating middle point of two vectors when put them into a same space. However as depicted in \eqref{eq:inner_product}, simple addition may introduce additional distortion because of nonalignment between models. Thus for comparison, we ran naked vector addition as well as vector addition with alignment using orthogonal transformation and low rank alignment.

  \subsubsection{Using PCA and projection}
  If vectors from two different models are just stacked together without any additional computation, all information from both models can be preserved. However because of the fact that we are using different part of same corpus, identical word does have same semantic within the same corpus. Also because semantic is under our hypothesis implied by the distribution of itself and it's context, only thing have to be focused on is the distribution of words and their context in different corpus. These distributions can be has similar with each other while some techniques preserving the distribution of sentences are employed when slicing the corpus.

\section{Experiment result}
In this paper, following the advice in \cite{levy2015improving}, hyper-parameters are chosen as \verb|win=10|, \verb|neg=5|, \verb|dim=500| and \verb|iter=5|. English wiki dump (enwiki-latest-pages-articles on March 2nd, 2016\footnote{from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}) is used as corpus. In table \ref{table:benchmark} the line "base-line" represents the performance of model who uses whole corpus with out any division or combination. each column stands for a evaluation dataset or training time. This benchmark is also kept the same with \cite{levy2015improving}.
\begin{table*}
\caption{Performance and training time of different scalable strategies}
\begin{tabular}{c|cccccccc|c}
\hline
Dataset   & AP   & MEN  & MTurk & WS353 & WS353R & WS353S & Google & MSR  & training time\ \hline
Base-line & .595 & .736 & .694  & .611  & .514   & .754   & .661   & .440 & 5d 3h 11m 25s\\ \hline
\end{tabular}
\label{table:benchmark}
\end{table*}

Because  rely substantially on the hypothesis that each word in vocabulary is independently distributed there is a be trained and maintained is inspired by Ising model, which description stochastic mechanic This is respired by the stochastic mechanic metaphor of distributive representation of words. Each corpus set could be treated as a thermal system 
\subsubsection{Direct combination on embedded vectors}
%%\subsubsubsection{Alignment of embedded chat}
%%\subsubsubsection{Form a new distribution}
\subsubsection{Training a new model using ensemble prediction}

\subsection{Optimization using variance reduced stochastic gradient descent(VRSGD)}
\subsubsection{Similarity between SG-MCMC and stochastic optimization}
\subsubsection{Asynchronous stochastic gradient descent with variance reduction for non-convex optimization}
\subsubsection{Inspiration from SVRG}
\subsection{Experiment results}

\bibliography{acl2017}
\bibliographystyle{acl_natbib}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
\grid
