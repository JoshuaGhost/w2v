\subsection{Dividing corpus}
	We can divide the corpora using two different ways, the following part of this sub-section will describe them briefly.
	
	\subsubsection{Interweaving divide}
	We can divide the corpus in such a interweaving way, that at first we denote sentences in all of documents in corpus with integers and if we divide the whole corpus into k pieces, where the t-th piece becomes all sentences that denoted as $dn+(t-1)$ (where $d\in Z_+$ and $d<k$). For example if we have the sentences numbered as 0, 1, 2, 3, 4, 5, ... in the 0th piece becomes 0, 2, 4, ... while the 1st becomes 1, 3, 5, ... Division using this way enables each piece contains different sentences, while spread each documentation into different pieces by granularity of sentence. In this way, each model trained from a sub-corpus describes a partial information of original corpus. And because a document in the original corpus contains most of the time more than one sentences, also under the hypothesis that one word in the same document means throughout the same, only perturbation instead of huge deviation between embedding vectors of sub-models can be introduced.
	
	\subsubsection{Division using random sampling}\label{sss:random_sampling}
	If maintenance of word distribution is required for the sake of preserving the semantic properties of words, a sampling method can be leveraged. Like described before, at first all of sentences contained in corpus are denoted by integers. Then for each one of \verb|d| sub-corpus, we sample $\frac{|D|}{d}$ sentences, where $|D|$ is the total number of sentences in corpus and like mentioned before, \verb|d| is the number of sub-corpora.

	While dividing the corpus into \verb|d| pieces, each sentence in the original corpus is sampled with probability $\frac{1}{d}$, so that important statistical properties of words and their context can vary. Denote the count of the central word $w_i$ within the original corpus as $#(w_i)$, while $#(w_i^p)$ represents count of this central word in the p-th sub-corpus. Considering the most common situation that each sentence may contain $w_i$ once or absolutely not. The $#(w_i)$ then equals to the count of sentences, where $w_i$ appears. If the sentences in a single sub-corpus are sampled from original corpus in probability of $\frac{1}{d}$, $#(w_i^p)$ is now $#(w_i)$. Analogously for $#(c_j)$ and $#(c_j^p)$ of context words $c_i$, as well as number of co-occurrence $#(w_i, c_j)$, $#(w_i^p, c_j^p)$ of $w_i$ and $c_j$.
