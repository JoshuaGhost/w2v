\subsection{Normalized or Unnormalized?}
According to work of Levy et al. \cite{levy2015improving}, normalization of each vector enables that every time when we calculate inner product of two normalized vectors, we are actually calculate the cosine similarity between them. Considering float number of dimensions composing original (unnormalized) vectors in each sub-model can scale differently, a normalization that every vector is kept to length 1 also makes it possible that when an orthogonal transformation-vector addition of two models is employed, the result will not trend to the model, whose vectors has higher L2-norm.

