\section{Experiment results} \label{experiment_results}
For this paper, we finished all experiments following the advice in \cite{levy2015improving}, thus hyper-parameters are chosen as \verb|win=10|, \verb|neg=5|, \verb|dim=500| and \verb|iter=5|. English wiki dump (enwiki-latest-pages-articles on March 2nd, 2016\footnote{from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}) is used as corpus. The result of experiments are written in \ref{sec:result}. In those tables the line "baseline" represents the performance of model who uses whole corpus with out any division or combination. the lines called "avg {skip,sampling}" represent the average performance over all fragments produced by two types of division methods. Each column in both tables stands for an evaluation dataset or time consumed for combination or (average) training. This benchmark is also kept the same with \cite{levy2015improving}. All of the experiments ran on 18 cores of a server with dual-way Intel(R) Xeon(R) CPU E5645 @ 2.40GHz with 126G of RAM. The size of entire corpus is approximately 14G. The combination and training times, however, because of public occupation of the server within institute, are for reference only. What is not mentioned in \ref{sec:result} is that the average division time for corpus. For \verb|skip| method this time is 2.487 sec, for \verb|sampling| method it is 41.878 sec.

From the result we can draw several conclusions.
\begin{itemize}
	\item \verb|sampling| method performs under nearly all circumstances better than \verb|skip| method,
	\item \verb|PCA|, without any \verb|LRA| or normalization, cooperating especially with \verb|seq| order, performs the best,
	\item \verb|LRA| can help improving result of direct vector addition,
	\item \verb|PCA| and orthogonal transformation should never occur together with \verb|LRA|.
\end{itemize}

The reason for the first conclusion is like inferred in section \ref{sec:division}, \verb|sampling| method preserves distribution over all words and thus produces model fragments more 'smoothly'. This consequentially represents less disturbance among model fragments and thence better performance when combining several model fragments together. For the second conclusion, from the table we can see that under both types of division method the configuration \verb|seq|-\verb|PCA| without \verb|LRA| or normalization seizures top-3 among all configurations on 10 out of 17 evaluation datasets. This also proves the hypothesis that compared with direct vector addition and vector addition after orthogonal transformation, PCA is the most optimal method while decreasing data-dimensions when most of dimensions provide no more information but redundancy. The \verb|seq| (consecutive order) however, doesn't help much when the combination process runs in a e.g. Map-Reduce cluster. Therefore binary order can be a secondary choice when large scale parallel computation is needed. As to the third conclusion, if a \verb|PCA| is too time and space consuming, considering the calculation demand of PCA or SVD, directly vector addition between two models can be a secondary substitution. A \verb|LRA| might help to improve the performance of vector addition because of the alignment, but \verb|LRA| itself is too time consuming and thus the profit doesn't cover the loss in the aspect of time. \verb|LRA| provide an alignment approach with help of low-rank-representation, which according to \cite{boucher2015aligning} guarantees only local linearity. Furthermore, according to the formulas in \verb|LRA| algorithm, it treats disturbance between data as noise and try to diminish it, which firstly do harm to information to both model fragments, which is already be done by PCA and orthogonal transformation\footnote{due to calculation accuracy of modern electronic computer} and secondly make our central thought of 'enrich the detail through combination of several model fragments' meaningless.