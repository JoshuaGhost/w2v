\section{Experiment results} \label{experiment_results}
In this paper, following the advice in \cite{levy2015improving}, hyper-parameters are chosen as \verb|win=10|, \verb|neg=5|, \verb|dim=500| and \verb|iter=5|. English wiki dump (enwiki-latest-pages-articles on March 2nd, 2016\footnote{from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2}) is used as corpus. In table \ref{table:benchmark} the line "base-line" represents the performance of model who uses whole corpus with out any division or combination. each column stands for a evaluation dataset or training time. This benchmark is also kept the same with \cite{levy2015improving}. All of the experiments run on $type of prometheus$ with 18 core activated. However because of common occupation of the server among institute, the training times are for reference only.

\begin{table*}
\caption{Performance and training time of different combination strategies}
\begin{tabular}{c|cccccccc|c}
\hline
Dataset   & AP   & MEN  & MTurk & WS353 & WS353R & WS353S & Google & MSR  & training time\ \tabularnewline \hline
Base-line & .595 & .736 & .694  & .611  & .514   & .754   & .661   & .440 & 5d 3h 11m 25s\\ \hline
\end{tabular}
\label{table:benchmark}
\end{table*}

From the result we can draw some conclusions. First of all is that $sample method$. Secondly pure PCA, especially cooperate with $order$, performs the best. This is mainly because that $reason$. If PCAs are too time consuming, a more efficient but a little bit less performing alternative can be $second alternative$. This configuration can get such nice score is mainly because $reason$. Furthermore, some by-product of the experiments are also interesting, for example $by-products$
