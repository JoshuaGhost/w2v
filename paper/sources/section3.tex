\section{System-level Optimization Methods}
Training a well-performed natural language model requires huge size of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4 days. An optimizing method or several methods are thus needed to be performed. Hyper-parameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyper-parameters also affect performance of a trained model. Therefore in order to shorten the training time while preserving performance, our strategy is either to find a trade-off for those parameters, or keep those hyper-parameters in the most performing way then parallelize the training process. Several methods are later on to be introduced and used to compare training time as well as performance with base-line model.

There are already several ways of parallel computation models of words embedding. For example the work of Erik Ordentlich et. al \cite{ordentlich2016network} figures out the bottleneck of building a distributed training system is the network overhead. The transferring of both output and input vectors of words to the word2vec client, as well as gradient to PS Shards, provoke huge network throughput. \citep{ordentlich2016network}'s solution to that is divide each of embedding vectors "vertically (divide a d-dimension vector into k d/k dimension vectors)" into several components, each of which is maintained by one of the PS Shards. Every time after each word2vec client select a central word with its context as well as negative samplings (e.g. in Skip-gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, avoiding transferring of intermediate result and reduce the training time.

Another paper from Ji, Shao et. al. \cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild \cite{recht2011hogwild} log-free strategy. Every time after a central word, its context word and negative samplings was selected, a dot product between embedding of context word and input weight of target(central) word and negative samples have to be calculated. in legacy Hogwild, negative samples and target words could be mutually different among different contexts. Under this circumstance the scalar product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negative samples. The scalar product is the between two matrices and BLAS level 3 functions could dedicate.

Contrastively, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an efficient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the conflict and increment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierarchical softmax can benefit due to its tree-type structure and  thus frequent access of the root.
