\section{Dedicate sub-models training and combination}
Unlike the systematic way mentioned in the section above, we present in this paper essentially such method that at first train dedicate sub-models separately, each of which consume part of corpus file, instead of the whole file. These processes can be run at different processors or even different mappers in a MapReduce cluster \cite{dean2008mapreduce}. Then these sub-models are combined with several different strategies. Due to \verb|iter| times passing-through of whole corpus in the traditional training process, it's trivial that the training time of a distributed representation model depends substantially on size of corpus. All of word vectors form an ambient Euclidean space, the purpose of distributed word representation is construction of a low-dimensional representation of one of its subspace \cite{Mahadevan2015reasoning}. Several "coarse" sub-models can be trained by using information from different parts of corpus. Which can be irrelevant with each other or be sampled so that the distribution of words are preserved. Then these sub-models can be combined using different strategies to reduce the "coarseness" of each single one of them. There are various of strategies can be conducted while dividing corpus, aligning of manifolds in each models and combining them together. In the following subsections we will go through them one by one.

\input{s3/subsection1.tex}
\input{s3/subsection2.tex}
\input{s3/subsection3.tex}
\input{s3/subsection4.tex}
\input{s3/subsection5.tex}
