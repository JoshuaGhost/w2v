\section{Brief introduction on word embedding with Skip-gram model}
As illustrated in \cite{mikolov2013distributed}, the goal of training process of a distributed representation model is to find a participate parameter combination for a 1-(hidden-)layer neural network. In this section we will go through how a distributed representation model of words as vectors is trained.

  \subsection{Notions and Conventions}
  \verb|V|: Set of all vocabularies. Before training the model using corpus, we need to identify all of the legal words, i.e. words do have meaning and are frequent. Normally a threshold $\tau_v$ is introduced, so that all the words occur less than $\tau_v$ times within whole corpus are pruned. For the sake of a systematic comparison, in our experiment we use vocabulary extracted from \verb|GoogleNews-vectors-negative300.bin|\footnote{found in https://code.google.com/archive/p/word2vec/}, whose count is approximately same as \cite{levy2015improving}.

  \verb|v|: Number of vocabulary, i.e. $|V|$.

  \verb|X|: Input model of the neural network, A matrix with shape of $1 \times v$ .

This input is represented by matrix $X$ with shape of $1\times v$, where $v$ is size of vocabulary. As the same name in \verb|gensim|, we now call the hidden layer \verb|syn0|, i.e. vectors in the embedded space, a matrix with shape of $v\times dim$ where $dim$ is the number of dimensions of embedded space. While the connection matrix between hidden layer and output layer is called \verb|W| or \verb|syn1neg|, also in consistence with in \verb|gensim|, and which is a $(n+1)\times dim$ matrix, $n$ is the number of negative samples, the $+1$ indicates the target word. With Skip-gram method in \cite{mikolov2013distributed} using a simplified variant of Noise Contrastive Estimation (NCE) \cite{gutmann2012noise}, the activation function in output layer is logit function: $y=\frac{1}{1+\exp(-syn0\times W^T)}$. In each round of training, we at first find the context word and target word in the same window, then after forward propagation with respect to network structure, the activation function calculates predicted labels, which can be treated as 'co-occurrence' between context word and target word, as well as 'co-occurrence' between negative sampled words and target word. Then the error are defined as difference between labels (context word = 1, negtive sampled word = 0) and predicted result, which is used to multiply with learning factor $\alpha$ in the backward propagation. These iterate until the result convergent.
