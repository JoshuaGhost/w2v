%
% File acl2016.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Accelerating Word2Vec training by leveraging parrallel computation of sub-models}

\author{Zijian Zhang \\
	Master of Science\\
	Informatik\\
	Leibniz Universit{\"a}t Hannover\\
	{\tt zhangzijian0523@gmail.com}\\ \And
	Avishek Anand\\
	Post Doctor\\
	L3S Research Center\\
	KBS Leibniz Universit{\"a}t Hannover\\
	{\tt test@dummy.com}\\
  }

\date{}

\begin{document}

\maketitle

\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between finishing training fast and performancing.
 
\end{abstract}


\section{Introduction}
In NLP applications, the first problem to solve is to find a propose representation (or token) of words\cite{schutze2008introduction}. One can use a random generated integer or even Haffman code\cite{elcompression} to do it, as long as that specific way of representation could be \'understood\' by machines. How ever, simple integer or frequency based representation lose the ordering or the words and they don't take semantics of words into consideration\cite{le2014distributed}. Distributed word representations, or word embeddings, is the way to have been successfully used in many NLP applications. 

This type of word representation dates back to 1986 due to Rumelhart, Hinton and Williams\cite{williams1986learning}. Word embedding uses co-occurrence of words and a softmax functional with help of stochastic gradient descent(SGD) to train a model, where each word is represented by, saying \'embedded in\' a vector in a high-dimensional space. In which model metric of distance (e.g. L2-Norm) between a pair of synonyms, or other pairs of words representing similiar concept is smaller than other pairs. Also those representation vectors of words have also \'parallelicity\' in word pairs, such that vec("Beijing")-vec("China")+vec("Germany") is close to vec("Berlin")\cite{le2014distributed}, which means vector from "China" to "Beijing" is somehow has the same direction and norm as the vector from "Germany" to "Berlin". Popular applications of word embedding are for example machine translation\cite{cho2014learning}, image annotation\cite{weston2011wsabie} and so on. 

Frameworks like gensim and word2vec library in TensorFlow by Google calculate those embeddings using association measures like Point-wise Mutual Information (PMI)\cite{church1990word} and better-performed Positive PMI (PPMI)\cite{bullinaria2007extracting} also Shifted PMI (SPPMI, first presented in \cite{goldberg2014word2vec}) considering the the fact that introduction of hyperparameter k will shift the to optimise PMI(w, c), where k is the number of negtive sample in Skip-gram with Negative Sampling and w is the central word and c is the nagtive sample. Techniques for generating lower-rank representations have also been employed, such as PPMI-SVD  and GloVe , both achieving state-of-the-art performance on a variety of tasks. Alternatively, vector-space models can be generated with \emph{predictive} methods, which generally outperform the count-based methods , the most notable of which is Skip-gram with Negative Sampling (SGNS) referred in \cite{mikolov2013distributed}, which uses a neural network with only one hidden-layer to generate embeddings. It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences. In this paper, the standard-line experiment is conducted using gensim with SGNS window sampling, negative sampling, and stochastic gradient descent (SGD) to minimize a loss function that defined on both co-occurence of words in a same window (positive co-occurence) and negative co-occurrence between central word of a window and negtive sample words. 

\section{Trainning-time Optimization Methods}
Training a well-performanced natural language model needs feeding of huge amount of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4.5 days. An optimizing method or several methods need to be performed. Hyperparameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyperparameters also affect performance of a trained model. Therefore in order to shorten the training time while perserving performance, either to find a trade-off for those parameters, or keep those hyperparameters in the most performansing way and parallelizing the training process. In this paper, we keep hyperparameters same as most-performed SGNS model in \cite{levy2015improving}, namely \verb|win=10|, \verb|neg=5|, \verb|dim=500|, \verb|iter=50| and use same vocabulary as GoogleNews-vectors-negative300.bin.gz found here\footnote{https://code.google.com/archive/p/word2vec/} also trained using google news and contain similar numbers of word like the paper mentioned abouve. Several method could be then introduced and used to make comparition of training time as well as performance with standard model.

\subsection{Optimization with respect to hardware}
There are around the world several ways of parallel computation of words embedding. For example the work of Erik Ordentlich et. al\cite{ordentlich2016network}. They point out the bottleneck of building a distributed training system is the network overhead. The transferation vectors of both output and input words to the word2vec client, as well as gradient in gradient descent to PS Shards, engenders huge network thoughput. Their solution for that is devide each of embedding vectors 'vertically' into several components, each of which is maintained by one of the PS Shards. Everytime after each word2vec client select a central word with its context as well as negtive samplings (e.g. in Skip-Gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, which avoids transferation of intermediate result and reduce the training time.

Another paper from Ji, Shao et. al.\cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild\cite{recht2011hogwild} log-free strategy. Everytime after a central word, its context word and negtive samplings was selected, a dot pruduct between embedding of context word and input weight of target(central) word and negtive samples have to be calculated. in legacy Hogwild, negtive samples and target words could be mutually different among different contexts. Under this circumstance the scala product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negtive samples. The scala product is the between two matrixes and BLAS level 3 functions could dedicate.

Alternatively, the work from Jeroen B.P. Vuurens et.al.\cite{eickhoff2016efficient} researches in the level of hardware and proposes an effecient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the confilit and increasment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierachical softmax can benifit due to its tree-type structure and  thus frequent access of the root.

\subsection{Combination of sub-models using low rank alignment(LRA)}
Unlike the practitional way mentioned in the section before, another approach of building words' distributed representation parallely was also inspired. Clearly we could know that the training time of a distributed representation model depened substaintially on size of corpus it use. Because afterall the training process goes through the whole corpus \verb|iter| times. So what if we at first devide corpus into several mutual exclusive parts, then train several sub-models separately, using different parts of corpus? As we know, the This is respired by the stochastic mechanic metaphor of distributive representation of words. Each corpus set could be treated as a thermal system 
\subsection{Optimazation using variance reduced stochastic gradient descent(VRSGD)}

\bibliography{acl2016}
\bibliographystyle{acl2016}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
