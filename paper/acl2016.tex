%
% File acl2016.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2016}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{Accelerating Word2Vec training by leveraging parrallel computation of sub-models}

\author{Zijian Zhang \\
	Master of Science\\
	Informatik\\
	Leibniz Universit{\"a}t Hannover\\
	{\tt zhangzijian0523@gmail.com}\\ \And
	Avishek Anand\\
	Post Doctor\\
	L3S Research Center\\
	KBS Leibniz Universit{\"a}t Hannover\\
	{\tt test@dummy.com}\\
  }

\date{}

\begin{document}

\maketitle

\begin{abstract}

    In this paper, we propose a method of distributed word embedding with the help of gensim. At first we calculating dedicated sub-models using parts of corpora, then we combine them using simply sorting over each embedding dimension or Low Rank Alignment(LRA). The separation and combination may do little harm to evaluation result, but the speed acceleration of our new distributing way of training model may provide a new trade-off argumentation between finishing training fast and performancing.
 
\end{abstract}


\section{Introduction}
In NLP applications, the first problem to solve is to find a propose representation (or token) of words \cite{schutze2008introduction}. One can use a random generated integer token, one-hot encoding \cite{turian2010word} or Haffman code \cite{elcompression}, with respect to data compression, deal with it, as long as the representation could be \'understood\' by machines. However, representations such as simple integer don't take the semantic information into consideration \cite{le2014distributed}. While method such as one-hot encoding will also cause \textit{curse of dimentionality} if being directly used in an NLP applications \cite{bengio2003neural}. Word representations with respect of semantic and within low dimension are therefore need to be developed. 

As human beings, we can understand the meaning of one word through looking for the corresponded entry in a dictionary and read the discription. In order to understand the discription we have to find a discription of the discription, i.e. meta-discription, meta-meta-discription and so on all the way run into the akward stymie of self-reference. It's alright for a human brain but a catestrophy for modern computers who are based on formal arithmetic system. This self-reference can't be complete and conflict-free according to G{\"o"}del's incompleteness theorems \cite{godel1931formal}.

Thanks for the contribution by Zellig Harris, a basic hypothesis on distributional semantics was introducd that liguistic items with similar distributions have simillar meanings. Meaning, or semantic similarity between two linguistic expression depends strongly on the circumstance or the context they appear \cite{harris1954distributional}. Fortunatly, computers are champions in forming distributional properties by counting and regression.

This word representation dates back to 1986 due to Rumelhart, Hinton and Williams \cite{williams1986learning}. Word embedding uses co-occurrence of words and a softmax functional with help of stochastic gradient descent(SGD) to train a model, where each word is represented by, saying \'embedded in\' a vector in a high-dimensional space. In which model metric of distance (e.g. L2-Norm) between a pair of synonyms, or other pairs of words representing similiar concept is smaller than other pairs. Also those representation vectors of words have also \'parallelicity\' in word pairs, such that vec("Beijing")-vec("China")+vec("Germany") is close to vec("Berlin") \cite{le2014distributed}, which means vector from "China" to "Beijing" is somehow has the same direction and norm as the vector from "Germany" to "Berlin". Popular applications of word embedding are for example machine translation \cite{cho2014learning}, image annotation \cite{weston2011wsabie} and so on. 

Frameworks like gensim and word2vec library in TensorFlow by Google calculate those representations using associative measures like 
Point-wise Mutual Information (PMI) \cite{church1990word}. Moreover, it was later on researched that introduction of Positive PMI (PPMI) \cite{bullinaria2007extracting} and Shifted PMI (SPPMI, first presented in \cite{goldberg2014word2vec}) improves performance of models, considering the the fact that introduction of hyperparameter \verb|k| will shift the to optimise \verb|PMI(w, c)|, where \verb|k| is the number of negtive samples in Skip-gram with Negative Sampling and \verb|w| is the central word and \verb|c| is the nagtive sample or context word. In order to dedct noise from generative models, Frameworks such as PPMI-SVD and GloVe employ techniques like lower-rank representation, both of which achieve performantly on a variety of tasks. Alternatively, vector-space models can be generated with \emph{predictive} methods, which generally outperform the count-based methods , the most notable of which is Skip-gram with Negative Sampling (SGNS) referred in \cite{mikolov2013distributed}, which uses a neural network with only one hidden-layer to generate embeddings. It implicitly factorizes a shifted PMI matrix, and its performance has been linked to the weighting of positive and negative co-occurrences. In this paper, the standard-line experiment is conducted using gensim with SGNS window sampling, negative sampling, and stochastic gradient descent (SGD) to minimize a loss function that defined on both co-occurence of words in a same window (positive co-occurence) and negative co-occurrence between central word of a window and negtive sample words. 


\section{Brief introduction on word embedding Skip-gram model}
As illustrated in \cite{mikolov2013distributed}, the goal of training process of a distributed representation model is to find a participate parameter combination for a 1-(hidden-)layer neural network. In this section we will go through how a distributed representation model of words as vectors is trained.

	\subsection{Notions and Conventions}
	\verb|V|: Set of all vocabularies. Before training the model using corpus, we need to identify all of the valible words, i.e. words do have meaning and are frequent. Normally a threshold $\tau_v$ is introduced, so that all the words occur less than $\tau_v$ times within whole corpus are pruned. For the sake of a systematic comparition, in our experiment we use vocabulary extracted from \verb|GoogleNews-vectors-negative300.bin|\footnote{found in https://code.google.com/archive/p/word2vec/}, whoes count is approximately same as \cite{levy2015improving}.

	\verb|v|: Number of vocabulary, i.e. $|V|$.

	\verb|X|: Input model of the neural network, A matrix with shape of $1 \cross v$ .

This input is represented by matrix $X$ with shape of $1\cross v$, where $v$ is size of vocabulary. As the same name in \verb|gensim|, we now call the hidden layer \verb|syn0|, i.e. vectors in the embedded space, a matrix with shape of $v\cross dim$ where $dim$ is the number of dimentions of embedded space. While the connection matrix between hidden layer and output layer is called \verb|W| or \verb|syn1neg|, also in consistence with in \verb|gensim|, and which is a $(n+1)\cross dim$ matrix, $n$ is the number of negtive samples, the $+1$ indicates the target word. With Skip-gram method in \cite{mikolov2013distributed} using a simplified variant of Noise Contrastive Estimation (NCE) \cite{gutmann2012noise}, the activation function in output layer is logit function: $y=\frac{1}{1+\exp(-syn0\cross W^T)}$. In each round of training, we at first find the context word and target word in the same window, then after forward propagation with respect to network structure, the activation function calculates predicted labels, which can be treated as 'cooccurrence' between context word and target word, as well as 'cooccurrence' between negtive sampled words and target word. Then the error are defined as difference between labels (context word = 1, negtive sampled word = 0) and predicted result, which is used to multiplicate with learning factor $\alpha$ in the backward propagation. These iterate until the result convergente.

\section{Trainning-time Optimization Methods}
Training a well-performanced natural language model needs feeding of huge amount of corpora. However, this process is highly time consuming. For example, the standard experiment mentioned in this paper spends approximately 4.5 days. An optimizing method or several methods need to be performed. Hyperparameters such as number of negative sampling, size of window, embedding dimension as well as minimum threshold of count of vocabulary all around corpora may do effect to training time. Also according to the paper \cite{levy2015improving}, these hyperparameters also affect performance of a trained model. Therefore in order to shorten the training time while perserving performance, either to find a trade-off for those parameters, or keep those hyperparameters in the most performansing way and parallelizing the training process. In this paper, we keep hyperparameters same as most-performed SGNS model in \cite{levy2015improving}, namely \verb|win=10|, \verb|neg=5|, \verb|dim=500|, \verb|iter=50| . Several method could be then introduced and used to make comparition of training time as well as performance with standard model.

	\subsection{Optimization with respect to hardware}
	There are around the world several ways of parallel computation of words embedding. For example the work of Erik Ordentlich et. al \cite{ordentlich2016network}. They point out the bottleneck of building a distributed training system is the network overhead. The transferation vectors of both output and input words to the word2vec client, as well as gradient in gradient descent to PS Shards, engenders huge network thoughput. Their solution for that is devide each of embedding vectors 'vertically' into several components, each of which is maintained by one of the PS Shards. Everytime after each word2vec client select a central word with its context as well as negtive samplings (e.g. in Skip-gram), instead of require embedding vectors from PS Shard, they use remote procedure call (RPC) to tell every PS Shard to calculate dot product and gradient locally, which avoids transferation of intermediate result and reduce the training time.

	Another paper from Ji, Shao et. al. \cite{ji2016parallelizing} provides a optimized scheme of leveraging BLAS level 3 function to accelerate forward propagation. Considering a word2vec training process using Hogwild \cite{recht2011hogwild} log-free strategy. Everytime after a central word, its context word and negtive samplings was selected, a dot pruduct between embedding of context word and input weight of target(central) word and negtive samples have to be calculated. in legacy Hogwild, negtive samples and target words could be mutually different among different contexts. Under this circumstance the scala product is only between a matrix and a vector, which means it can only leverage level 2 BLAS to accelerate. The paper propose that we could shard all the context words together in every single batch, using same target(central) word and negtive samples. The scala product is the between two matrixes and BLAS level 3 functions could dedicate.

	Alternatively, the work from Jeroen B.P. Vuurens et.al. \cite{eickhoff2016efficient} researches in the level of hardware and proposes an effecient way of using high-speed cache of CPU. They find the level 3 BLAS presented by Ji et.al. implicitly lowers the number of times writing and reading shared memory, which consequently provokes the confilit and increasment of queuing time because of concurrent access of the same memory. They found if an effective cache strategy is exploited, hierachical softmax can benifit due to its tree-type structure and  thus frequent access of the root.

	\subsection{Combining parallel trained sub-models}
	Unlike the practitional way mentioned in the section before, we present in this paper especially a way of combining parallel trained sub-models. It's clearly to see that the training time of a distributed representation model depends substaintially on size of its corpus. Afterall the training process goes through the whole corpus \verb|iter| times. Then what if we at first devide corpus into several mutual exclusive parts, then train several sub-models separately, using different parts of corpus, then combine them together? To reach this goal, we have to at first analyze what happend during training process of a distributed representation model.

According to the Beyesian theory


Because  rely substentially on the hypothesis that each word in vocabulary is independently distributedthere is a be trained and matained is inspired by Ising model, which discription stochastic mechanic This is respired by the stochastic mechanic metaphor of distributive representation of words. Each corpus set could be treated as a thermal system 
\subsubsection{Direct combination on embedded vectors}
\subsubsubsection{Alignment of embedded chat}
\subsubsubsection{Form a new distribution}
\subsubsection{Training a new model using ensembled prediction}

\subsection{Optimazation using variance reduced stochastic gradient descent(VRSGD)}
\subsubsection{Similarity between SG-MCMC and stochastic optimization}
\subsubsection{Asynchronous stochastic gradient descent with variance reduction for non-convex optimization}
\subsubsection{Inspiration from SVRG}

\bibliography{acl2016}
\bibliographystyle{acl2016}

\appendix

\section{Supplemental Material}
\label{sec:supplemental}

\section{Multiple Appendices}
\dots can be gotten by using more than one section. We hope you won't
need that.

\end{document}
